# -*- coding: utf-8 -*-
"""otml final with map.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q4n0HR3JEBUQx6b1x0-_O6QVW5wraiN_
"""

pip install osmnx pandas

import osmnx as ox
import pandas as pd
from shapely.geometry import Point

# =========================================================================
# 1. CONFIGURATION (Constants)
# =========================================================================

place_name = "Delhi, India"

target_names = [
    "Red Fort",
    "Old Parliament House",
    "India Gate",
    "Qutb Minar",
    "Supreme Court of India",
    "Lotus Temple",
    "Embassy of the United States of America",
    "All India Institute of Medical Sciences",
    "New Delhi Railway Station",
    "Connaught Place"
]

hvt_tags = {
    "historic": ["fort", "monument", "memorial", "archaeological_site"],
    "office": ["government", "diplomatic"],
    "tourism": ["attraction"],
    "amenity": ["courthouse", "hospital", "station"],
    "place": ["suburb"],
    "landuse": ["education"]
}
# =========================================================================
# 2. EXTRACTION AND CLEANING LOGIC
# =========================================================================

def extract_and_process_hvts():
    """
    1. Queries OSM for features based on general tags.
    2. Filters the results down to the specific target names.
    3. Isolates the geographic center (centroid) for each target.
    4. Saves the clean Name, Longitude, and Latitude to a CSV file.
    """
    print(f"Starting data extraction within: {place_name}")
    try:
        # Step 1: Query the general data from OpenStreetMap
        gdf = ox.features.features_from_place(place_name, tags=hvt_tags)
    except Exception as e:
        print(f"An error occurred during the OSMnx query: {e}")
        print("Please check your 'place_name' and network connection.")
        return pd.DataFrame()

    # Reset index to make 'osmid' usable as a regular column
    gdf = gdf.reset_index()

    # Step 2: Filter results down to specific target names and isolate one point per name
    final_hvts = gdf[gdf['name'].isin(target_names)].copy()

    # CRITICAL FIX: Drop multiple geometries (like points, lines, polygons)
    # that share the same name, keeping the first instance found.
    final_hvts.drop_duplicates(subset=['name'], keep='first', inplace=True)

    # Step 3: Convert the geometry into a single point coordinate (Lon, Lat)
    hvt_points = []

    for _, row in final_hvts.iterrows():
        geom = row['geometry']

        if geom is None:
            continue

        # Standardize location to the centroid (center point) for all geometry types
        if geom.geom_type in ['Polygon', 'MultiPolygon', 'LineString', 'MultiLineString', 'Point']:
            center = geom.centroid
        else:
            # Fallback for unexpected types
            center = geom.centroid

        # Only extract the necessary data: Name, Lon, Lat
        hvt_points.append({
            'name': row['name'],
            'lon': center.x,  # Longitude (x)
            'lat': center.y   # Latitude (y)
        })

    df_hvt = pd.DataFrame(hvt_points)

    # Step 4: Save the clean, filtered data to CSV
    if not df_hvt.empty:
        output_filename = "critical_targets_coords.csv"
        df_hvt.to_csv(output_filename, index=False)
        print("\n--- CSV Generation Complete ---")
        print(f"Successfully isolated {len(df_hvt)} unique critical target points.")
        print(f"Clean coordinates saved to {output_filename}")
        print("\nContents of the CSV:")
        print(df_hvt)
    else:
        print("\n--- CSV Generation Failed ---")
        print("No unique critical target points were found matching the names provided.")

    return df_hvt

# Execute the extraction process
if __name__ == "__main__":
    extract_and_process_hvts()

import numpy as np
import matplotlib.pyplot as plt
import random
import pandas as pd
from shapely.geometry import Point # Shapely is needed here for converting geometry

# Note: osmnx is no longer needed in this file since we are loading the CSV.

# =========================================================================
# 1. PROJECT CONSTANTS (Agreement between Role A and Role B)
# =========================================================================

# Geometric & Budget Constraints
GRID_SIZE = 100  # The simulation area is 100x100 units (0 to 100)
N_SENSORS = 10   # N: Number of sensors (dimensions of a single position vector)
SENSOR_RANGE = 15 # R: Coverage radius of each sensor

# PSO Hyperparameters
SWARM_SIZE = 40  # P: Number of particles (candidate solutions)
MAX_GENERATIONS = 500  # G: How long the simulation runs

# PSO Physics/Velocity Update Parameters (W, C1, C2)
W = 0.75   # Inertia Weight
C1 = 2.0   # Cognitive Coefficient
C2 = 2.0   # Social Coefficient
MAX_VELOCITY = 10.0

# --- Configuration Section for Role A's Data Input and Weights ---

# The CSV containing the coordinates of the 7-10 targets you found
INPUT_CSV_FILE = "critical_targets_coords.csv"

# Criticality Weights and Buffer Sizes for targets found in the CSV
CRITICALITY_TIERS = {
    # Tier 1: Highest Risk, Largest Buffer
    "Red Fort": {"weight": 10, "buffer": 7},
    "Supreme Court of India": {"weight": 10, "buffer": 7},
    "New Delhi Railway Station": {"weight": 10, "buffer": 7}, # Assuming this was the missing one

    # Tier 2: High Risk, Standard Buffer
    "Old Parliament House": {"weight": 9, "buffer": 5},
    "India Gate": {"weight": 8, "buffer": 5},
    "All India Institute of Medical Sciences": {"weight": 8, "buffer": 5},
    "Embassy of the United States of America": {"weight": 9, "buffer": 5},

    # Tier 3: Medium Risk, Standard Buffer
    "Lotus Temple": {"weight": 6, "buffer": 4},
    "Qutb Minar": {"weight": 6, "buffer": 4},
    "Connaught Place": {"weight": 5, "buffer": 5},

    # Default for any missing or unlisted target
    "DEFAULT": {"weight": 3, "buffer": 3}
}
# -------------------------------------------------------------------------

# =========================================================================
# 2. DATA PROCESSING & MAP CREATION (Role A - Final Criticality Input)
# =========================================================================

def load_hvt_coordinates():
    """Loads the clean coordinates from the CSV file."""
    try:
        df_hvt = pd.read_csv(INPUT_CSV_FILE)
        print(f"Loaded {len(df_hvt)} HVT coordinates from {INPUT_CSV_FILE}.")
        return df_hvt
    except FileNotFoundError:
        print(f"Error: Critical file '{INPUT_CSV_FILE}' not found.")
        return None
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return None

def create_criticality_map():
    """
    Creates the 100x100 Criticality Map (W_j) by normalizing coordinates
    and applying variable weights and buffers.
    """
    df_hvt = load_hvt_coordinates()
    W_map = np.ones((GRID_SIZE, GRID_SIZE), dtype=float) * 1  # Base weight 1 everywhere

    if df_hvt is None or df_hvt.empty:
        print("Warning: Using dummy map with only a center hotspot.")
        W_map[40:60, 40:60] = 10
        return W_map

    # 1. Determine the bounding box of the extracted targets
    min_lon, max_lon = df_hvt['lon'].min(), df_hvt['lon'].max()
    min_lat, max_lat = df_hvt['lat'].min(), df_hvt['lat'].max()

    # 2. Map coordinates to the 0-100 grid space (Normalization)
    # This transforms real-world coordinates into simulation coordinates.
    df_hvt['grid_x'] = ((df_hvt['lon'] - min_lon) / (max_lon - min_lon)) * GRID_SIZE
    df_hvt['grid_y'] = ((df_hvt['lat'] - min_lat) / (max_lat - min_lat)) * GRID_SIZE

    print("Applying variable weights and buffers...")

    for _, row in df_hvt.iterrows():
        x, y = int(row['grid_x']), int(row['grid_y'])
        name = row['name']

        # --- YOUR IMPLEMENTATION OF VARIABLE BUFFER LOGIC ---

        # Get weight and buffer size based on the name, defaulting to Tier 3 if name not found
        tier_data = CRITICALITY_TIERS.get(name, CRITICALITY_TIERS["DEFAULT"])
        weight = tier_data["weight"]
        buffer_size = tier_data["buffer"]

        # Apply the weight and a buffer zone to the map
        x_min = max(0, x - buffer_size // 2)
        x_max = min(GRID_SIZE, x + buffer_size // 2 + 1)
        y_min = max(0, y - buffer_size // 2)
        y_max = min(GRID_SIZE, y + buffer_size // 2 + 1)

        # Ensure the new weight is higher than the existing value (if targets overlap)
        # This prevents a low-weight target from overwriting a high-weight target zone.
        W_map[x_min:x_max, y_min:y_max] = np.maximum(W_map[x_min:x_max, y_min:y_max], weight)
        # ---------------------------------------------------

    print(f"Criticality Map (W_j) created. Max weight: {W_map.max()}")
    return W_map

def load_criticality_map():
    """Wrapper to call map creation."""
    return create_criticality_map()

# =========================================================================
# 3. CORE FITNESS FUNCTION (The Geometric Scorecard)
# =========================================================================

grid_x, grid_y = np.meshgrid(np.arange(GRID_SIZE), np.arange(GRID_SIZE))
GRID_POINTS = np.vstack([grid_x.flatten(), grid_y.flatten()]).T

def calculate_fitness(position_vector, W_map, R):
    """
    The FINAL Weighted Coverage Fitness Function using NumPy.
    This replaces complex Shapely Area of Union with fast grid-based summation.
    """
    sensors = position_vector.reshape(N_SENSORS, 2)

    # Calculate distance from EVERY grid point to EVERY sensor (10000 x N_SENSORS array)
    distances_sq = ((GRID_POINTS[:, np.newaxis] - sensors) ** 2).sum(axis=2)

    # Find the minimum distance to the nearest sensor for each grid point (10000 array)
    min_distances = np.sqrt(distances_sq.min(axis=1))

    # Determine Coverage: Covered if distance < R (15 units)
    is_covered = (min_distances <= R).astype(int)

    # Calculate Weighted Coverage Score: SUM(W_j * Coverage_j)
    W_flat = W_map.flatten()
    weighted_score = np.sum(W_flat * is_covered)

    return weighted_score

# =========================================================================
# 4. PSO IMPLEMENTATION (Role B - Core Algorithm)
# =========================================================================

class Particle:
    """Represents a single candidate solution (10 sensor placements)."""
    def __init__(self, bounds):
        self.position = np.random.uniform(bounds[0], bounds[1], size=2 * N_SENSORS)
        self.velocity = np.random.uniform(-MAX_VELOCITY, MAX_VELOCITY, size=2 * N_SENSORS)
        self.fitness = -np.inf
        self.pbest_position = self.position.copy()
        self.pbest_fitness = -np.inf

class PSO:
    """The main Particle Swarm Optimization engine."""
    def __init__(self, bounds):
        self.bounds = bounds
        self.swarm = [Particle(bounds) for _ in range(SWARM_SIZE)]
        self.gbest_position = self.swarm[0].position.copy()
        self.gbest_fitness = -np.inf
        # CRITICAL: Load the map here!
        self.W_map = load_criticality_map()

    def initialize_swarm(self):
        # ... (Initialization logic remains the same)
        for particle in self.swarm:
            fitness = calculate_fitness(particle.position, self.W_map, SENSOR_RANGE)
            particle.fitness = fitness
            particle.pbest_fitness = fitness

            if fitness > self.gbest_fitness:
                self.gbest_fitness = fitness
                self.gbest_position = particle.position.copy()

    def update_velocity(self, particle):
        # ... (Velocity update logic remains the same)
        r1 = np.random.rand(2 * N_SENSORS)
        r2 = np.random.rand(2 * N_SENSORS)

        inertia = W * particle.velocity
        cognitive = C1 * r1 * (particle.pbest_position - particle.position)
        social = C2 * r2 * (self.gbest_position - particle.position)

        new_velocity = inertia + cognitive + social

        particle.velocity = np.clip(new_velocity, -MAX_VELOCITY, MAX_VELOCITY)

    def update_position(self, particle):
        # ... (Position update logic remains the same)
        particle.position += particle.velocity
        particle.position = np.clip(particle.position, self.bounds[0], self.bounds[1])

    def run_optimization(self):
        # ... (Optimization loop logic remains the same)
        gbest_history = []

        print("\n--- Starting PSO Optimization ---")

        for g in range(MAX_GENERATIONS):
            for particle in self.swarm:
                # 1. Update Velocity and Position
                self.update_velocity(particle)
                self.update_position(particle)

                # 2. Evaluate Fitness
                new_fitness = calculate_fitness(particle.position, self.W_map, SENSOR_RANGE)

                # 3. Update Personal Best (pbest)
                if new_fitness > particle.pbest_fitness:
                    particle.pbest_fitness = new_fitness
                    particle.pbest_position = particle.position.copy()

                # 4. Update Global Best (gbest)
                if new_fitness > self.gbest_fitness:
                    self.gbest_fitness = new_fitness
                    self.gbest_position = particle.position.copy()

            # Record the best score of this generation
            gbest_history.append(self.gbest_fitness)

            if (g + 1) % 50 == 0 or g == 0:
                print(f"Gen {g+1}/{MAX_GENERATIONS}: GBest Score = {self.gbest_fitness:.2f}")

        print("\n--- Optimization Finished ---")
        return self.gbest_position, self.gbest_fitness, gbest_history

# =========================================================================
# 5. EXECUTION & RESULTS (Phase III)
# =========================================================================

def run_pso_project():
    # 1. Setup and Initialization
    bounds = (0, GRID_SIZE)
    pso_engine = PSO(bounds)
    pso_engine.initialize_swarm()

    # 2. Run Optimization Loop
    optimal_placement, max_fitness, history = pso_engine.run_optimization()

    # 3. Results Processing
    optimal_sensors = optimal_placement.reshape(N_SENSORS, 2)

    print("\n=========================================================================")
    print("FINAL OPTIMIZATION RESULTS")
    print("=========================================================================")
    print(f"MAXIMUM WEIGHTED COVERAGE SCORE: {max_fitness:.2f}")
    print(f"Optimal Sensor Coordinates (N={N_SENSORS}):")
    for i, (x, y) in enumerate(optimal_sensors):
        print(f"Sensor {i+1}: ({x:.2f}, {y:.2f})")

    # 4. Visualization
    plt.figure(figsize=(12, 5))

    # A. Convergence Plot
    plt.subplot(1, 2, 1)
    plt.plot(history, label="Global Best Fitness")
    plt.title("PSO Convergence Curve (Fitness over Generations)")
    plt.xlabel("Generation")
    plt.ylabel("Weighted Coverage Score")
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()

    # B. Placement Map
    plt.subplot(1, 2, 2)

    # Show the Criticality Map (Input)
    plt.imshow(pso_engine.W_map.T, origin='lower', cmap='hot',
               extent=[0, GRID_SIZE, 0, GRID_SIZE], interpolation='nearest')
    plt.colorbar(label='Criticality Weight ($W_j$)')

    # Overlay the Optimal Sensor Placement (Output)
    plt.scatter(optimal_sensors[:, 0], optimal_sensors[:, 1],
                s=150, c='cyan', edgecolors='black', label='Optimal Sensor Placement (N=10)', zorder=2)

    # Draw the sensor coverage circles (Optional, for visual clarity)
    for x, y in optimal_sensors:
        circle = plt.Circle((x, y), SENSOR_RANGE, color='cyan', alpha=0.1, linewidth=0)
        plt.gca().add_patch(circle)

    plt.title("Optimal Sensor Placement on Criticality Map")
    plt.xlabel("Grid X Coordinate")
    plt.ylabel("Grid Y Coordinate")
    plt.legend()
    plt.tight_layout()
    plt.show()

# Run the main project function
if __name__ == "__main__":
    run_pso_project()

import numpy as np
import matplotlib.pyplot as plt
import random
import pandas as pd
from shapely.geometry import Point # Shapely is needed here for converting geometry
import os # Import os for checking file existence

# Note: osmnx is no longer needed in this file since we are loading the CSV.

# =========================================================================
# 1. PROJECT CONSTANTS
# =========================================================================

GRID_SIZE = 100
N_SENSORS = 10
SENSOR_RANGE = 15

SWARM_SIZE = 40
MAX_GENERATIONS = 500

W = 0.75
C1 = 2.0
C2 = 2.0
MAX_VELOCITY = 10.0


INPUT_CSV_FILE = "critical_targets_coords.csv"

CRITICALITY_TIERS = {
    "Red Fort": {"weight": 10, "buffer": 7},
    "Supreme Court of India": {"weight": 10, "buffer": 7},
    "New Delhi Railway Station": {"weight": 10, "buffer": 7},

    "Old Parliament House": {"weight": 9, "buffer": 5},
    "India Gate": {"weight": 8, "buffer": 5},
    "All India Institute of Medical Sciences": {"weight": 8, "buffer": 5},
    "Embassy of the United States of America": {"weight": 9, "buffer": 5},

    "Lotus Temple": {"weight": 6, "buffer": 4},
    "Qutb Minar": {"weight": 6, "buffer": 4},
    "Connaught Place": {"weight": 5, "buffer": 5},

    "DEFAULT": {"weight": 3, "buffer": 3}
}

# =========================================================================
# 2. DATA PROCESSING & MAP CREATION (Role A - Final Criticality Input)
# =========================================================================

def load_hvt_coordinates():
    """Loads the clean coordinates from the CSV file."""
    if not os.path.exists(INPUT_CSV_FILE):
        print(f"Error: Critical file '{INPUT_CSV_FILE}' not found. Please run data_extractor.py first.")
        return None

    try:
        df_hvt = pd.read_csv(INPUT_CSV_FILE)
        print(f"Loaded {len(df_hvt)} HVT coordinates from {INPUT_CSV_FILE}.")
        return df_hvt
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return None

def create_criticality_map():
    """
    Creates the 100x100 Criticality Map (W_j) by normalizing coordinates
    and applying variable weights and buffers.
    """
    df_hvt = load_hvt_coordinates()
    W_map = np.ones((GRID_SIZE, GRID_SIZE), dtype=float) * 1

    if df_hvt is None or df_hvt.empty:
        print("Warning: Using dummy map with only a center hotspot.")
        W_map[40:60, 40:60] = 10
        return W_map

    min_lon, max_lon = df_hvt['lon'].min(), df_hvt['lon'].max()
    min_lat, max_lat = df_hvt['lat'].min(), df_hvt['lat'].max()

    if (max_lon - min_lon) == 0: max_lon += 0.0001
    if (max_lat - min_lat) == 0: max_lat += 0.0001

    df_hvt['grid_x'] = ((df_hvt['lon'] - min_lon) / (max_lon - min_lon)) * GRID_SIZE
    df_hvt['grid_y'] = ((df_hvt['lat'] - min_lat) / (max_lat - min_lat)) * GRID_SIZE

    print("Applying variable weights and buffers...")

    for _, row in df_hvt.iterrows():
        x, y = int(np.clip(row['grid_x'], 0, GRID_SIZE - 1)), int(np.clip(row['grid_y'], 0, GRID_SIZE - 1))
        name = row['name']

        tier_data = CRITICALITY_TIERS.get(name, CRITICALITY_TIERS["DEFAULT"])
        weight = tier_data["weight"]
        buffer_size = tier_data["buffer"]

        x_min = max(0, x - buffer_size // 2)
        x_max = min(GRID_SIZE, x + buffer_size // 2 + 1)
        y_min = max(0, y - buffer_size // 2)
        y_max = min(GRID_SIZE, y + buffer_size // 2 + 1)

        W_map[x_min:x_max, y_min:y_max] = np.maximum(W_map[x_min:x_max, y_min:y_max], weight)


    print(f"Criticality Map (W_j) created. Max weight: {W_map.max():.2f}")
    return W_map

def load_criticality_map():
    """Wrapper to call map creation."""
    return create_criticality_map()

# =========================================================================
# CORE FITNESS FUNCTION (The Geometric Scorecard)
# =========================================================================

grid_x, grid_y = np.meshgrid(np.arange(GRID_SIZE), np.arange(GRID_SIZE))
GRID_POINTS = np.vstack([grid_x.flatten(), grid_y.flatten()]).T

def calculate_fitness(position_vector, W_map, R):
    """
    The FINAL Weighted Coverage Fitness Function using NumPy.
    This replaces complex Shapely Area of Union with fast grid-based summation.
    """
    sensors = position_vector.reshape(N_SENSORS, 2)

    distances_sq = ((GRID_POINTS[:, np.newaxis] - sensors) ** 2).sum(axis=2)

    min_distances = np.sqrt(distances_sq.min(axis=1))

    is_covered = (min_distances <= R).astype(int)

    W_flat = W_map.flatten()
    weighted_score = np.sum(W_flat * is_covered)

    return weighted_score

# =========================================================================
# PSO IMPLEMENTATION
# =========================================================================

class Particle:
    """Represents a single candidate solution (10 sensor placements)."""
    def __init__(self, bounds):
        self.position = np.random.uniform(bounds[0], bounds[1], size=2 * N_SENSORS)
        self.velocity = np.random.uniform(-MAX_VELOCITY, MAX_VELOCITY, size=2 * N_SENSORS)
        self.fitness = -np.inf
        self.pbest_position = self.position.copy()
        self.pbest_fitness = -np.inf

class PSO:
    """The main Particle Swarm Optimization engine."""
    def __init__(self, bounds):
        self.bounds = bounds
        self.swarm = [Particle(bounds) for _ in range(SWARM_SIZE)]
        self.gbest_position = self.swarm[0].position.copy()
        self.gbest_fitness = -np.inf
        self.W_map = load_criticality_map()

    def initialize_swarm(self):
        for particle in self.swarm:
            fitness = calculate_fitness(particle.position, self.W_map, SENSOR_RANGE)
            particle.fitness = fitness
            particle.pbest_fitness = fitness

            if fitness > self.gbest_fitness:
                self.gbest_fitness = fitness
                self.gbest_position = particle.position.copy()

    def update_velocity(self, particle):
        r1 = np.random.rand(2 * N_SENSORS)
        r2 = np.random.rand(2 * N_SENSORS)

        inertia = W * particle.velocity
        cognitive = C1 * r1 * (particle.pbest_position - particle.position)
        social = C2 * r2 * (self.gbest_position - particle.position)

        new_velocity = inertia + cognitive + social

        particle.velocity = np.clip(new_velocity, -MAX_VELOCITY, MAX_VELOCITY)

    def update_position(self, particle):
        particle.position += particle.velocity
        particle.position = np.clip(particle.position, self.bounds[0], self.bounds[1])

    def run_optimization(self):
        gbest_history = []

        print("\n--- Starting PSO Optimization ---")

        for g in range(MAX_GENERATIONS):
            for particle in self.swarm:

                self.update_velocity(particle)
                self.update_position(particle)

                new_fitness = calculate_fitness(particle.position, self.W_map, SENSOR_RANGE)

                if new_fitness > particle.pbest_fitness:
                    particle.pbest_fitness = new_fitness
                    particle.pbest_position = particle.position.copy()

                if new_fitness > self.gbest_fitness:
                    self.gbest_fitness = new_fitness
                    self.gbest_position = particle.position.copy()

            gbest_history.append(self.gbest_fitness)

            if (g + 1) % 50 == 0 or g == 0:
                print(f"Gen {g+1}/{MAX_GENERATIONS}: GBest Score = {self.gbest_fitness:.2f}")

        print("\n--- Optimization Finished ---")
        return self.gbest_position, self.gbest_fitness, gbest_history

# =========================================================================
#  EXECUTION & RESULTS
# =========================================================================

def run_pso_project():
    bounds = (0, GRID_SIZE)
    pso_engine = PSO(bounds)
    pso_engine.initialize_swarm()

    optimal_placement, max_fitness, history = pso_engine.run_optimization()

    optimal_sensors = optimal_placement.reshape(N_SENSORS, 2)

    print("\n=========================================================================")
    print("FINAL OPTIMIZATION RESULTS SAVED")
    print("=========================================================================")
    print(f"MAXIMUM WEIGHTED COVERAGE SCORE: {max_fitness:.2f}")

    np.savez('pso_results.npz',
             optimal_sensors=optimal_sensors,
             max_fitness=max_fitness,
             W_map=pso_engine.W_map,
             history=history,
             GRID_SIZE=GRID_SIZE,
             SENSOR_RANGE=SENSOR_RANGE)

    print("Optimization data and map saved to pso_results.npz")

if __name__ == "__main__":
    run_pso_project()

import numpy as np
import matplotlib.pyplot as plt
import os

# =========================================================================
# 1. VISUALIZATION
# =========================================================================

def visualize_results():
    """
    Loads data from pso_results.npz and generates the final visualization.
    """
    results_file = 'pso_results.npz'

    if not os.path.exists(results_file):
        print(f"Error: Results file '{results_file}' not found.")
        print("Please run 'pso_optimizer.py' first to generate the optimization data.")
        return

    try:
        data = np.load(results_file, allow_pickle=True)

        optimal_sensors = data['optimal_sensors']
        max_fitness = data['max_fitness']
        W_map = data['W_map']
        history = data['history']
        GRID_SIZE = data['GRID_SIZE']
        SENSOR_RANGE = data['SENSOR_RANGE']
        N_SENSORS = len(optimal_sensors)

        print(f"\n--- Visualization Data Loaded ---")
        print(f"Final Weighted Score: {max_fitness:.2f}")
        print(f"Number of Sensors (N): {N_SENSORS}")
        print(f"Sensor Range (R): {SENSOR_RANGE}")

    except Exception as e:
        print(f"Error loading or extracting data from {results_file}: {e}")
        return

    # --- Visualization ---

    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    ax_convergence = axes[0]
    ax_convergence.plot(history, label="Global Best Fitness", color='teal', linewidth=2)
    ax_convergence.set_title(f"PSO Convergence: Max Fitness = {max_fitness:.2f}", fontsize=14)
    ax_convergence.set_xlabel("Generation", fontsize=12)
    ax_convergence.set_ylabel("Weighted Coverage Score", fontsize=12)
    ax_convergence.grid(True, linestyle='--', alpha=0.7)
    ax_convergence.legend(loc='lower right')

    ax_map = axes[1]

    im = ax_map.imshow(W_map.T, origin='lower', cmap='hot',
                       extent=[0, GRID_SIZE, 0, GRID_SIZE], interpolation='nearest')

    cbar = fig.colorbar(im, ax=ax_map, fraction=0.046, pad=0.04)
    cbar.set_label('Criticality Weight ($W_j$)', fontsize=12)

    ax_map.scatter(optimal_sensors[:, 0], optimal_sensors[:, 1],
                   s=200, c='lime', edgecolors='black', label='Optimal Sensor Placement (N=10)', zorder=2)

    for x, y in optimal_sensors:
        circle = plt.Circle((x, y), SENSOR_RANGE, color='lime', alpha=0.1, linewidth=1, edgecolor='darkgreen', zorder=1)
        ax_map.add_patch(circle)

    ax_map.set_title("Optimal Sensor Placement on Criticality Map", fontsize=14)
    ax_map.set_xlabel("Grid X Coordinate", fontsize=12)
    ax_map.set_ylabel("Grid Y Coordinate", fontsize=12)
    ax_map.legend(loc='upper right')
    ax_map.set_aspect('equal', adjustable='box')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    visualize_results()

import folium
import pandas as pd
import numpy as np
from folium import Circle, Marker

# =========================================================================
# CONFIGURATION - Must match your PSO code
# =========================================================================
GRID_SIZE = 100
N_SENSORS = 10
SENSOR_RANGE = 15  # Grid units
INPUT_CSV_FILE = "critical_targets_coords.csv"

def create_interactive_map(optimal_sensors_grid):
    """
    Creates an interactive Folium map showing:
    - Critical targets (HVTs)
    - Optimal sensor placements with coverage circles
    - Transforms grid coordinates back to real lat/lon

    Parameters:
    - optimal_sensors_grid: Nx2 array of sensor positions in grid coordinates (0-100)
    """

    # Load the original HVT data
    df_hvt = pd.read_csv(INPUT_CSV_FILE)

    # Get the bounding box (same normalization as your PSO code)
    min_lon, max_lon = df_hvt['lon'].min(), df_hvt['lon'].max()
    min_lat, max_lat = df_hvt['lat'].min(), df_hvt['lat'].max()

    # Calculate map center
    center_lat = (min_lat + max_lat) / 2
    center_lon = (min_lon + max_lon) / 2

    # Create the base map
    m = folium.Map(
        location=[center_lat, center_lon],
        zoom_start=12,
        tiles='OpenStreetMap'
    )

    # Add satellite view option
    folium.TileLayer('Cartodb dark_matter', name='Dark Mode').add_to(m)

    # =========================================================================
    # 1. Plot Critical Targets (HVTs)
    # =========================================================================
    for idx, row in df_hvt.iterrows():
        folium.Marker(
            location=[row['lat'], row['lon']],
            popup=f"<b>{row['name']}</b>",
            tooltip=row['name'],
            icon=folium.Icon(color='red', icon='exclamation-triangle', prefix='fa')
        ).add_to(m)

    # =========================================================================
    # 2. Transform Grid Coordinates Back to Real Lat/Lon
    # =========================================================================
    # Reverse the normalization: grid_coord = (real - min) / (max - min) * GRID_SIZE
    # Therefore: real = (grid_coord / GRID_SIZE) * (max - min) + min

    sensor_lons = (optimal_sensors_grid[:, 0] / GRID_SIZE) * (max_lon - min_lon) + min_lon
    sensor_lats = (optimal_sensors_grid[:, 1] / GRID_SIZE) * (max_lat - min_lat) + min_lat

    # =========================================================================
    # 3. Calculate Real-World Sensor Range (in meters)
    # =========================================================================
    # Convert grid units to real-world distance
    # Approximate: 1 degree latitude ‚âà 111 km
    lat_range = max_lat - min_lat
    lon_range = max_lon - min_lon

    # Average meters per grid unit
    meters_per_grid_unit_lat = (lat_range * 111000) / GRID_SIZE
    meters_per_grid_unit_lon = (lon_range * 111000 * np.cos(np.radians(center_lat))) / GRID_SIZE
    meters_per_grid_unit = (meters_per_grid_unit_lat + meters_per_grid_unit_lon) / 2

    sensor_range_meters = SENSOR_RANGE * meters_per_grid_unit

    # =========================================================================
    # 4. Plot Sensors with Coverage Circles
    # =========================================================================
    for i, (lon, lat) in enumerate(zip(sensor_lons, sensor_lats)):
        # Coverage circle
        folium.Circle(
            location=[lat, lon],
            radius=sensor_range_meters,
            color='cyan',
            fill=True,
            fillColor='cyan',
            fillOpacity=0.15,
            weight=2,
            popup=f"Sensor {i+1} Coverage<br>Range: {sensor_range_meters:.0f}m"
        ).add_to(m)

        # Sensor marker
        folium.CircleMarker(
            location=[lat, lon],
            radius=8,
            popup=f"<b>Sensor {i+1}</b><br>Lat: {lat:.5f}<br>Lon: {lon:.5f}",
            tooltip=f"Sensor {i+1}",
            color='black',
            fillColor='cyan',
            fillOpacity=0.9,
            weight=2
        ).add_to(m)

    # Add layer control
    folium.LayerControl().add_to(m)

    # Add title
    title_html = '''
    <div style="position: fixed;
                top: 10px; left: 50px; width: 400px; height: 60px;
                background-color: white; border:2px solid grey; z-index:9999;
                font-size:16px; padding: 10px">
    <b>Delhi Critical Infrastructure - Optimal Sensor Placement</b><br>
    <span style="font-size:12px">üî¥ Critical Targets | üîµ Sensor Coverage</span>
    </div>
    '''
    m.get_root().html.add_child(folium.Element(title_html))

    # Save the map
    output_file = "delhi_sensor_placement_map.html"
    m.save(output_file)
    print(f"\n‚úÖ Interactive map saved as: {output_file}")
    print(f"üìç {N_SENSORS} sensors placed covering {len(df_hvt)} critical targets")
    print(f"üìè Sensor range: ~{sensor_range_meters:.0f} meters")

    return m

# =========================================================================
# 5. INTEGRATION WITH YOUR PSO CODE
# =========================================================================
def integrate_with_pso_results(optimal_placement):
    """
    Call this function after your PSO optimization completes.

    Usage in your main code:
    optimal_placement, max_fitness, history = pso_engine.run_optimization()
    optimal_sensors = optimal_placement.reshape(N_SENSORS, 2)
    integrate_with_pso_results(optimal_sensors)
    """
    create_interactive_map(optimal_placement)

# =========================================================================
# STANDALONE TESTING (if you want to test with dummy data)
# =========================================================================
if __name__ == "__main__":
    # Example: Load your optimized sensor positions
    # Replace this with actual data from your PSO output

    print("To use this with your PSO code, add this line after optimization:")
    print("integrate_with_pso_results(optimal_sensors)")
    print("\nOr manually provide sensor coordinates:")

    # Example dummy sensors for testing (replace with your actual results)
    dummy_sensors = np.random.uniform(0, GRID_SIZE, size=(N_SENSORS, 2))
    create_interactive_map(dummy_sensors)